{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "20608825",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÅ Scanning training data from: D:/Study/Coding/Project/GAIT/processed_dataset_72x72\n",
      "‚úÖ Found 8091 non-empty sequences for the training set.\n",
      "üìÅ Scanning testing data from: D:/Study/Coding/Project/GAIT/processed_dataset_72x72\n",
      "‚úÖ Found 5402 non-empty sequences for the testing set.\n",
      "\n",
      "--- DataLoaders created successfully ---\n",
      "Batch of sequences shape: torch.Size([64, 1, 30, 72, 72])\n",
      "Batch of labels shape:   torch.Size([64])\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import numpy as np\n",
    "\n",
    "class Gait3DDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Custom PyTorch Dataset for the CASIA-B gait dataset.\n",
    "    This class loads data in sequences (e.g., 30 frames) for a 3D CNN.\n",
    "    \"\"\"\n",
    "    def __init__(self, root_dir, is_train, sequence_length=30, transform=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.is_train = is_train\n",
    "        self.sequence_length = sequence_length\n",
    "        self.transform = transform\n",
    "        \n",
    "        self.train_sequences = {'nm-01', 'nm-02', 'nm-03', 'nm-04', 'bg-01', 'cl-01'}\n",
    "        self.test_sequences = {'nm-05', 'nm-06', 'bg-02', 'cl-02'}\n",
    "        \n",
    "        self.sequence_paths = []\n",
    "        self.labels = []\n",
    "        \n",
    "        self._load_dataset()\n",
    "\n",
    "    def _load_dataset(self):\n",
    "        data_type = 'training' if self.is_train else 'testing'\n",
    "        print(f\"üìÅ Scanning {data_type} data from: {self.root_dir}\")\n",
    "        target_sequences = self.train_sequences if self.is_train else self.test_sequences\n",
    "\n",
    "        if not os.path.isdir(self.root_dir):\n",
    "            raise FileNotFoundError(f\"‚ùå Error: Directory not found at '{self.root_dir}'\")\n",
    "\n",
    "        all_subject_ids = sorted([d for d in os.listdir(self.root_dir) if os.path.isdir(os.path.join(self.root_dir, d))])\n",
    "        self.label_encoder = LabelEncoder()\n",
    "        self.label_encoder.fit(all_subject_ids)\n",
    "\n",
    "        for subject_id in all_subject_ids:\n",
    "            subject_path = os.path.join(self.root_dir, subject_id)\n",
    "            for sequence_type in sorted(os.listdir(subject_path)):\n",
    "                if sequence_type in target_sequences:\n",
    "                    sequence_path = os.path.join(subject_path, sequence_type)\n",
    "                    for angle in sorted(os.listdir(sequence_path)):\n",
    "                        angle_path = os.path.join(sequence_path, angle)\n",
    "                        if not os.path.isdir(angle_path): continue\n",
    "                        \n",
    "                        frame_files = [f for f in os.listdir(angle_path) if f.lower().endswith('.png')]\n",
    "                        \n",
    "                        if len(frame_files) > 0:\n",
    "                            self.sequence_paths.append(angle_path)\n",
    "                            self.labels.append(subject_id)\n",
    "                        \n",
    "        print(f\"‚úÖ Found {len(self.sequence_paths)} non-empty sequences for the {data_type} set.\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sequence_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        angle_path = self.sequence_paths[idx]\n",
    "        label_str = self.labels[idx]\n",
    "        \n",
    "        frame_files = sorted([os.path.join(angle_path, f) for f in os.listdir(angle_path) if f.lower().endswith('.png')])\n",
    "        \n",
    "        if not frame_files:\n",
    "            print(f\"‚ö†Ô∏è Warning: Found empty sequence at {angle_path}. Loading first item instead.\")\n",
    "            return self.__getitem__(0) \n",
    "        \n",
    "        indices = np.linspace(0, len(frame_files) - 1, self.sequence_length, dtype=int)\n",
    "        selected_frames = [frame_files[i] for i in indices]\n",
    "        \n",
    "        sequence = []\n",
    "        for frame_path in selected_frames:\n",
    "            image = cv2.imread(frame_path, cv2.IMREAD_GRAYSCALE)\n",
    "            if self.transform:\n",
    "                image = self.transform(image)\n",
    "            sequence.append(image)\n",
    "            \n",
    "        sequence_tensor = torch.cat(sequence, dim=0).unsqueeze(0) \n",
    "        \n",
    "        label = self.label_encoder.transform([label_str])[0]\n",
    "        \n",
    "        return sequence_tensor, torch.tensor(label, dtype=torch.long)\n",
    "\n",
    "# --- Main execution block to create DataLoaders ---\n",
    "if __name__ == '__main__':\n",
    "    BATCH_SIZE = 64\n",
    "    NUM_WORKERS = 0 # Keep at 0 for Jupyter\n",
    "    \n",
    "    data_transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "    ])\n",
    "\n",
    "    # ---------------------------------------------------\n",
    "    # THE FIX IS HERE: Corrected variable name\n",
    "    # ---------------------------------------------------\n",
    "    train_dataset = Gait3DDataset(\n",
    "        root_dir='D:/Study/Coding/Project/GAIT/processed_dataset_72x72', \n",
    "        is_train=True, \n",
    "        sequence_length=30, \n",
    "        transform=data_transform\n",
    "    )\n",
    "    \n",
    "    train_loader = DataLoader(\n",
    "        dataset=train_dataset, \n",
    "        batch_size=BATCH_SIZE, \n",
    "        shuffle=True, \n",
    "        num_workers=NUM_WORKERS,\n",
    "        pin_memory=True\n",
    "    )\n",
    "\n",
    "    # --- Create Testing Dataset and DataLoader ---\n",
    "    test_dataset = Gait3DDataset(\n",
    "        root_dir='D:/Study/Coding/Project/GAIT/processed_dataset_72x72', \n",
    "        is_train=False, \n",
    "        sequence_length=30, \n",
    "        transform=data_transform\n",
    "    )\n",
    "    \n",
    "    test_loader = DataLoader(\n",
    "        dataset=test_dataset, \n",
    "        batch_size=BATCH_SIZE, \n",
    "        shuffle=False, \n",
    "        num_workers=NUM_WORKERS,\n",
    "        pin_memory=True\n",
    "    )\n",
    "    \n",
    "    print(\"\\n--- DataLoaders created successfully ---\")\n",
    "    \n",
    "    try:\n",
    "        images, labels = next(iter(train_loader))\n",
    "        print(f\"Batch of sequences shape: {images.shape}\")\n",
    "        print(f\"Batch of labels shape:   {labels.shape}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading batch: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2552cf15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simple3DCNN(\n",
      "  (conv_stack): Sequential(\n",
      "    (0): Conv3d(1, 16, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
      "    (1): ReLU()\n",
      "    (2): BatchNorm3d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (3): MaxPool3d(kernel_size=(2, 2, 2), stride=(2, 2, 2), padding=0, dilation=1, ceil_mode=False)\n",
      "    (4): Conv3d(16, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
      "    (5): ReLU()\n",
      "    (6): BatchNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (7): MaxPool3d(kernel_size=(2, 2, 2), stride=(2, 2, 2), padding=0, dilation=1, ceil_mode=False)\n",
      "    (8): Conv3d(32, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
      "    (9): ReLU()\n",
      "    (10): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (11): MaxPool3d(kernel_size=(2, 2, 2), stride=(2, 2, 2), padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (classifier): Sequential(\n",
      "    (0): Flatten(start_dim=1, end_dim=-1)\n",
      "    (1): Linear(in_features=15552, out_features=1024, bias=True)\n",
      "    (2): ReLU()\n",
      "    (3): Dropout(p=0.5, inplace=False)\n",
      "    (4): Linear(in_features=1024, out_features=512, bias=True)\n",
      "    (5): ReLU()\n",
      "    (6): Dropout(p=0.5, inplace=False)\n",
      "    (7): Linear(in_features=512, out_features=123, bias=True)\n",
      "  )\n",
      ")\n",
      "\n",
      "Shape of the model output: torch.Size([4, 123])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class Simple3DCNN(nn.Module):\n",
    "    \"\"\"\n",
    "    A 3D CNN with a more robust classifier to prevent bottlenecks.\n",
    "    \"\"\"\n",
    "    def __init__(self, num_classes):\n",
    "        super(Simple3DCNN, self).__init__()\n",
    "        \n",
    "        # Input shape: [B, 1, 30, 72, 72]\n",
    "        self.conv_stack = nn.Sequential(\n",
    "            nn.Conv3d(in_channels=1, out_channels=16, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm3d(16), \n",
    "            nn.MaxPool3d(kernel_size=(2, 2, 2)), # Output: [B, 16, 15, 36, 36]\n",
    "            \n",
    "            nn.Conv3d(in_channels=16, out_channels=32, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm3d(32), \n",
    "            nn.MaxPool3d(kernel_size=(2, 2, 2)), # Output: [B, 32, 7, 18, 18]\n",
    "            \n",
    "            nn.Conv3d(in_channels=32, out_channels=64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm3d(64), \n",
    "            nn.MaxPool3d(kernel_size=(2, 2, 2))  # Output: [B, 64, 3, 9, 9]\n",
    "        )\n",
    "        \n",
    "        # Calculate the flattened size once\n",
    "        flat_size = 64 * 3 * 9 * 9 # 15552\n",
    "\n",
    "        # -----------------------------------------------------------------\n",
    "        # THE FIX: A deeper classifier to handle the 15,552 features\n",
    "        # -----------------------------------------------------------------\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Flatten(), \n",
    "            \n",
    "            nn.Linear(in_features=flat_size, out_features=1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=0.5),\n",
    "\n",
    "            nn.Linear(in_features=1024, out_features=512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=0.5),\n",
    "\n",
    "            nn.Linear(in_features=512, out_features=num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv_stack(x)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "# --- Main block to instantiate the model ---\n",
    "if __name__ == '__main__':\n",
    "    # Get the number of classes from the train_dataset\n",
    "    NUM_CLASSES = len(train_dataset.label_encoder.classes_) \n",
    "    \n",
    "    # Create the model\n",
    "    model = Simple3DCNN(num_classes=NUM_CLASSES)\n",
    "    \n",
    "    # Print the model architecture\n",
    "    print(model)\n",
    "    \n",
    "    # Test with a dummy input tensor\n",
    "    dummy_input = torch.randn(4, 1, 30, 72, 72) \n",
    "    output = model(dummy_input)\n",
    "    \n",
    "    print(f\"\\nShape of the model output: {output.shape}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "59ce9812",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "\n",
      "üß† Starting training for 10 epochs...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10 [Training]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 127/127 [06:10<00:00,  2.92s/it, Loss=4.6928]\n",
      "Epoch 1/10 [Testing]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 85/85 [04:17<00:00,  3.03s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1 Summary:\n",
      "  Train Loss: 4.9518 | Train Acc: 1.2483%\n",
      "  Test Loss:  4.5850 | Test Acc:  3.6283%\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/10 [Training]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 127/127 [06:19<00:00,  2.99s/it, Loss=4.4437]\n",
      "Epoch 2/10 [Testing]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 85/85 [03:39<00:00,  2.58s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 2 Summary:\n",
      "  Train Loss: 4.5266 | Train Acc: 3.0528%\n",
      "  Test Loss:  4.3923 | Test Acc:  5.1648%\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/10 [Training]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 127/127 [05:35<00:00,  2.64s/it, Loss=4.2598]\n",
      "Epoch 3/10 [Testing]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 85/85 [03:29<00:00,  2.46s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 3 Summary:\n",
      "  Train Loss: 4.3445 | Train Acc: 4.1033%\n",
      "  Test Loss:  3.9548 | Test Acc:  9.0707%\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/10 [Training]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 127/127 [05:26<00:00,  2.57s/it, Loss=4.3833]\n",
      "Epoch 4/10 [Testing]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 85/85 [03:14<00:00,  2.29s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 4 Summary:\n",
      "  Train Loss: 4.1461 | Train Acc: 5.7348%\n",
      "  Test Loss:  3.8741 | Test Acc:  11.4402%\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/10 [Training]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 127/127 [05:26<00:00,  2.57s/it, Loss=4.1281]\n",
      "Epoch 5/10 [Testing]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 85/85 [03:18<00:00,  2.34s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 5 Summary:\n",
      "  Train Loss: 3.9322 | Train Acc: 8.0336%\n",
      "  Test Loss:  3.6382 | Test Acc:  15.2536%\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/10 [Training]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 127/127 [05:19<00:00,  2.52s/it, Loss=4.1629]\n",
      "Epoch 6/10 [Testing]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 85/85 [03:12<00:00,  2.27s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 6 Summary:\n",
      "  Train Loss: 3.7781 | Train Acc: 9.8999%\n",
      "  Test Loss:  3.5282 | Test Acc:  17.5305%\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/10 [Training]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 127/127 [05:23<00:00,  2.55s/it, Loss=4.0231]\n",
      "Epoch 7/10 [Testing]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 85/85 [03:17<00:00,  2.33s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 7 Summary:\n",
      "  Train Loss: 3.6493 | Train Acc: 11.0864%\n",
      "  Test Loss:  3.2733 | Test Acc:  22.5657%\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/10 [Training]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 127/127 [05:17<00:00,  2.50s/it, Loss=3.4951]\n",
      "Epoch 8/10 [Testing]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 85/85 [03:34<00:00,  2.52s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 8 Summary:\n",
      "  Train Loss: 3.5480 | Train Acc: 12.6931%\n",
      "  Test Loss:  3.1060 | Test Acc:  23.8800%\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/10 [Training]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 127/127 [05:16<00:00,  2.49s/it, Loss=3.2113]\n",
      "Epoch 9/10 [Testing]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 85/85 [03:18<00:00,  2.33s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 9 Summary:\n",
      "  Train Loss: 3.3903 | Train Acc: 15.0908%\n",
      "  Test Loss:  2.9402 | Test Acc:  29.5446%\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/10 [Training]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 127/127 [05:16<00:00,  2.49s/it, Loss=3.3708]\n",
      "Epoch 10/10 [Testing]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 85/85 [03:11<00:00,  2.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 10 Summary:\n",
      "  Train Loss: 3.2821 | Train Acc: 17.5504%\n",
      "  Test Loss:  2.9954 | Test Acc:  29.0263%\n",
      "------------------------------------------------------------\n",
      "‚úÖ Finished Training in 90.11 minutes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "from tqdm import tqdm # For the progress bar\n",
    "import time\n",
    "\n",
    "# ==================================================================\n",
    "#  1. CONFIGURATION AND SETUP\n",
    "# ==================================================================\n",
    "NUM_EPOCHS = 10\n",
    "LEARNING_RATE = 0.001\n",
    "\n",
    "# --- Setup Device, Loss Function, and Optimizer ---\n",
    "# (This assumes your 'model', 'train_loader', and 'test_loader' are ready)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "model.to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "# ==================================================================\n",
    "#  2. TRAINING & EVALUATION LOOP\n",
    "# ==================================================================\n",
    "print(f\"\\nüß† Starting training for {NUM_EPOCHS} epochs...\")\n",
    "start_time = time.time()\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    # --- Training Phase ---\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    train_correct = 0\n",
    "    train_total = 0\n",
    "    \n",
    "    # Use tqdm for a progress bar on the training loop\n",
    "    train_pbar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{NUM_EPOCHS} [Training]\")\n",
    "    \n",
    "    for images, labels in train_pbar:\n",
    "        # 'images' now has the shape [B, 1, 30, 72, 72]\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images) # The 3D model processes the 3D data\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        train_total += labels.size(0)\n",
    "        train_correct += (predicted == labels).sum().item()\n",
    "        \n",
    "        train_pbar.set_postfix({'Loss': f'{loss.item():.4f}'})\n",
    "\n",
    "    # --- Validation/Testing Phase ---\n",
    "    model.eval()\n",
    "    test_loss = 0.0\n",
    "    test_correct = 0\n",
    "    test_total = 0\n",
    "    \n",
    "    test_pbar = tqdm(test_loader, desc=f\"Epoch {epoch+1}/{NUM_EPOCHS} [Testing]\")\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_pbar:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            \n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            test_loss += loss.item()\n",
    "            \n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            test_total += labels.size(0)\n",
    "            test_correct += (predicted == labels).sum().item()\n",
    "\n",
    "    # --- Print a summary for the epoch ---\n",
    "    avg_train_loss = train_loss / len(train_loader)\n",
    "    train_accuracy = 100 * train_correct / train_total\n",
    "    avg_test_loss = test_loss / len(test_loader)\n",
    "    test_accuracy = 100 * test_correct / test_total\n",
    "    \n",
    "    print(f\"\\nEpoch {epoch+1} Summary:\")\n",
    "    print(f\"  Train Loss: {avg_train_loss:.4f} | Train Acc: {train_accuracy:.4f}%\")\n",
    "    print(f\"  Test Loss:  {avg_test_loss:.4f} | Test Acc:  {test_accuracy:.4f}%\")\n",
    "    print(\"-\" * 60)\n",
    "\n",
    "total_time = time.time() - start_time\n",
    "print(f\"‚úÖ Finished Training in {total_time/60:.2f} minutes.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f630c99",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "725aad45",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2116ff8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9862f5b7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f524ee06",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb0c7f99",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16c70586",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2303d8b1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6333946",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
